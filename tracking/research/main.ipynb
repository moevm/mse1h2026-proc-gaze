{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e97e43f",
   "metadata": {},
   "source": [
    "* [pygazer](https://www.pygaze.org/about/) - аналог webgazer для python, устаревшая библиотека для python 2.7(пофиксить ошибки + запустить не получилось)\n",
    "* [openCV-gaze-tracking](https://learnopencv.com/gaze-tracking/) - используют дополнительно ИК камеры для отслеживания глаз\n",
    "* [camera-to-screen](https://github.com/pperle/gaze-tracking-pipeline/tree/main#) - openCV + pytorch, кажется, рабочий пайплайн: на видео с камеры выделяется лицо, глаза и [точки на лице](https://github.com/google-ai-edge/mediapipe/wiki/MediaPipe-Face-Mesh), подаются на вход сверточной нейросети(в данной работе используются предобученные слои VGG16 на imagenet), которая предсказывает 3D вектор направления взгляда(gaze vector), затем этот вектор проецируется на экран в координаты (x, y) (калибровка, например, с помощью полиномиальной регрессии), в конце в идеале добавить фильтрацию(например, фильтр Калмана), чтобы вз1гляд не дрожал на экране.\n",
    "* [Webcam-based gaze estimation for computer screen](https://gazerecorder.com/wp-content/uploads/2024/04/frobt-11-1369566-1.pdf) - прочитал основную часть статьи, этот подход кажется проще, чем предыдущий, хотя они очень похожи. В данной статье берут готовую модель \"gaze-estimation-adas-0002\" из [библиотеки](https://github.com/openvinotoolkit/openvino) для CV от Intel, которая на входе получает обрезанную часть картинки с правым глазом, с левым глазом и вектор положения головы(3 угла), а на выходе дает вектор направления взгляда единичной длины. Далее в статье показывают, как с помощью матрицы однородного преобразования(афинное без растягиваний), проецируется вектор направления взгляда в точку на экране, где расстояние от глаз до экрана оценивается с помощью регрессии(данные берутся на этапе калибровки для каждого отдельно взятого человека, в статье использовали 4 точки по разным углам монитора)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3facaee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "core = ov.Core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2154f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to download models VPN needed\n",
    "# pip install openvino-dev\n",
    "# omz_downloader --name <name-of-the-model> \n",
    "\n",
    "FACE_MODEL_PTH             = r\"../../intel/face-detection-0200/FP32/face-detection-0200.xml\"\n",
    "HEAD_POSE_MODEL_PTH        = r\"../../intel/head-pose-estimation-adas-0001/FP32/head-pose-estimation-adas-0001.xml\"\n",
    "FACIAL_LANDMARKS_MODEL_PTH = r\"../../intel/landmarks-regression-retail-0009/FP32/landmarks-regression-retail-0009.xml\"\n",
    "GAZE_ESTIMATION_MODEL_PTH  = r\".../../intel/gaze-estimation-adas-0002/FP32/gaze-estimation-adas-0002.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7da159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_model = core.read_model(FACE_MODEL_PTH)\n",
    "compiled_face_detection_model = core.compile_model(face_detection_model, device_name=\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b958d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = compiled_face_detection_model.input(0).shape\n",
    "model_h, model_w = input_shape[2], input_shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed9c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "INPUT_IMG_PTH  = r\"images/img.png\"\n",
    "OUTPUT_IMG_PTH = r\"images/result.png\"\n",
    "\n",
    "img = cv2.imread(INPUT_IMG_PTH)\n",
    "initial_height, initial_width = img.shape[:2]\n",
    "\n",
    "resized = cv2.resize(img, (model_w, model_h))\n",
    "input_image = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n",
    "input_image = np.transpose(input_image, (2, 0, 1)) # C W H\n",
    "input_image = np.expand_dims(input_image, axis=0) # N C W H\n",
    "\n",
    "results = compiled_face_detection_model(input_image)\n",
    "output = results[compiled_face_detection_model.output(0)].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cff2c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faces found: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "THRESHOLD = 0.5\n",
    "\n",
    "faces_count = 0\n",
    "cropped_faces = []\n",
    "for _, _, confidence, x_min, y_min, x_max, y_max in output:\n",
    "    \n",
    "    if confidence < THRESHOLD:\n",
    "        break\n",
    "    \n",
    "    x_min, y_min = int(x_min * initial_width), int(y_min * initial_height)\n",
    "    x_max, y_max = int(x_max * initial_width), int(y_max * initial_height)\n",
    "    \n",
    "    color = (0, 255, 0) # green, BGR\n",
    "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color, thickness=2)\n",
    "    cropped_face = img[y_min:y_max, x_min:x_max]\n",
    "    cropped_faces.append(cropped_face)\n",
    "    \n",
    "    label = f\"Face: {confidence:.2f}\"\n",
    "    cv2.putText(img, label, (x_min, y_min - 10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    faces_count += 1\n",
    "\n",
    "print(f\"Faces found: {faces_count}\")\n",
    "cv2.imwrite(OUTPUT_IMG_PTH, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4fa82eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "facial_landmarks_model = core.read_model(FACIAL_LANDMARKS_MODEL_PTH)\n",
    "compiled_landmarks_model = core.compile_model(facial_landmarks_model, \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "565acc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "face1 = cropped_faces[0]\n",
    "initial_height, initial_width = face1.shape[:2]\n",
    "\n",
    "input_shape = compiled_landmarks_model.input(0).shape\n",
    "model_h, model_w = input_shape[2], input_shape[3]\n",
    "\n",
    "input_image = np.resize(face1, (model_h, model_w, 3))\n",
    "input_image = np.transpose(input_image, (2, 0, 1))\n",
    "input_image = np.expand_dims(input_image, axis=0)\n",
    "results = compiled_landmarks_model(input_image)\n",
    "output = results[compiled_landmarks_model.output(0)].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6a44424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 76\n",
      "119 84\n",
      "48 111\n",
      "19 152\n",
      "112 160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(0, len(output), 2):\n",
    "    x, y = int(output[i] * initial_width), int(output[i+1] * initial_height)\n",
    "    \n",
    "    color = (0, 0, 255)\n",
    "    print(x, y)\n",
    "    cv2.circle(face1, (x, y), 1, color, 10)\n",
    "\n",
    "cv2.imwrite(OUTPUT_IMG_PTH, face1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
